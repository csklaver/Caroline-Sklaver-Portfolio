# Hi, I'm Caroline! Welcome to my portfolio of Data Science Projects!

>

## **George Washington Univeristy** 
### M.S. Data Science

&nbsp;

## [Capstone Project](https://github.com/csklaver/Capstone-Group6)
### *Feature Engineering and Missing Value Imputation Methods for Classification of Depression with Deep Neural Networks*

> PyCharm, Jupyter Notebook, NumPy, Pandas, scikit-learn, Keras, Matplotlib, Seaborn, Tableau,

[Exploratory Data Analysis & Trends](https://csklaver.github.io/)

This project explores missing value imputation methods, resampling techniques, feature engineering, and neural networks to predicting depression using NHANES data from 2005 to 2016. Statistical and machine learning imputation techniques along with feature engineering techniques can improve classification results of both a deep ANN and a multi-layer CNN. NHANES data is highly imbalanced with respect to depression (7.49% positive class) and contains many missing values. The primary goal of this project is to identify the best imputation and feature engineering methods to optimize classification ability of these DNNs with the challenges of a highly imbalanced dataset and a target disease with complex causes.

*Feature Engineering, MLP & KNN progressive imputation, Deep Artificial Neural Network, Convolutional Neural Network*

&nbsp;
## [Machine Learning Project](https://github.com/csklaver/ML_Kiva_Crowdfunding)
### *Kiva Crowdfunding: A Machine Learning Analysis*
> Google Colab, NumPy, Pandas, scikit-learn, Matplotlib, Seaborn 

The aim of this project is to apply machine learning methods to gain insight into how Kiva determines the amount (USD - regression) and repayment interval type (monthly, bullet, or irregular - Classification) of each loan in Latin America. To achieve these goals we will preprocess the data (merging, cleaning, handling identifiers and missing values, etc.), perform exploratory data analyses, split the data into training, validation, and testing sets, chose a variety of algorithms for both regression and classification, and fine-tune the hyperparameters of those models. In classifying repayment interval, we achieved an F1-score of 0.874 with Histogram Gradient Boosting, followed by Random Forest and MLPC. According to our Random Forest Classifier, the top 5 features given in order of importance are: Loan Term (months), Country Colombia, Loan Amount, Agriculture and Livestock Activity, and MPI. In all, our regression models predicting loan amount were weak, thus we cannot make conclusions about the features with respect to loan amount. On the other hand, our classification models were quite accurate in predicting repayment interval type, which may be useful in determining what a borrowers repayment interval should be, given all other factors.

*Multiple Linear Regression, Logistic Regression, Decision Trees, Random Forests, Extreme Gradient Boosting, Histogram Gradient Boosting, Multi-Layer Perceptron*


&nbsp;
## [Data Mining Project](https://github.com/csklaver/Data-Mining_GUI-Analysis)
### *Analysis of London Bike Share Usage: A Graphical User Interface*
> PyCharm, NumPy, Pandas, scikit-learn, PyQt5, Matplotlib, Seaborn

The purpose of this project is to perform EDA and machine learning algorithms to predict 2015 London bike share activity. This project includes python script to preprocess and export the cleaned data, as well as create visualizations for our final presentation. We also include a GUI portion for users to run through EDA, Linear Regressions, and Multiple Linear Regressions with this dataset.

*Muliple Linear Regressions, Correlations*

&nbsp;
## [Natural Language Processing Project](https://github.com/csklaver/NLP_The-2020-Presidential-Race)
### *The 2020 Presidential Race: NLP and Sentiment Analysis of Online News Articles*
> Jupyter Notebook, Pandas, NLTK, SpaCy, Genism, Textacy, scikit-learn

The aim of this project was to determine the sentiment of top online news outlets towards the top three presidential candidates, Trump, Biden, and Sanders. Pre-processed text by tokenizing, lemmatizing, and removing stop words. Examined term frequenices and TF-IDF scores for each candidate and each news outlet. Lastly, we developed and evaluated SVM and XGBoost classification models with varying weights (binary word count, term frequency, and TF-IDF score) and n-grams (unigram, bigram, and trigram). The best configuration was achieved using SVM using unigrams and TF-IDF scores as weights.

*Term Frequencies, TF-IDF Scores, Extreme Gradient Boosting, Support Vector Machines*

&nbsp;
## [Network Data Science Project](https://github.com/csklaver/network_science_flights) 
### *Making Connections: An Analysis of US Commercial Aviation Networks*
> RStudio, dplyr, igraph, ggplot2

Explored and analyzed the US commercial airline transportation networks from 1998 to 2018 measured by betweenness, centrality, diameter, and degree. Created maps and vizualizations of the network nodes and edges. Conducted a case study of Southwest Airlines and Delta which revealed the differences in growth strategies; Southwest focusing on including more airports increasing betweenness and Delta focusing on growing degree of major hubs. Lastly, we found the total network to be robust when removing 20 major hubs (in the case of COVID-19), but unable to handle the same volume of passengers.


&nbsp;
## [Data Visualization Project](https://csklaver.github.io/DATS6401-Individual-Project/)
### *Top Country Health, Education, and Military Spending: A Visual Analysis*
> HTML, CSS, JS, Google API Charts

This projet explores the healthcare, education, and military spending of some of the top countries in the world. This includes 15 of the G-20 countries from 2013 to 2018. Utilizing Google API Charts, I explore trends over time and compare spending across top countries.<br/>

&nbsp;
&nbsp;
>

## **Princeton Univeristy**
### B.A. Ecology & Evolutionary Biology
&nbsp;
## [Senior Thesis Project](https://github.com/csklaver/Princeton_Thesis)
### *The Major Factors Driving Global Antibiotic Consumption: A Quantitative Analysis of the Leading Determinants from 2000 to 2015*
> RStudio, stats, ggbiplot, ggplot2, plotly, stargazer

The goal of this project is to identify the socio-economic and cultural factors driving global antibiotic consumption. Data on total antibiotic consumption and consumption by class was provided by the Center for Disease Dynamics, Economics & Policy, while most other variables were obtained from the World Development Indicators in the World Bank Databank. To analyze the data, I used principle component analysis biplots, variance inflation factors, and multiple linear regressions. In low- and middle-income countries, the use of basic sanitation facilities and the density of pharmaceutical personnel were found to have significant negative correlations to antibiotic consumption across classes. In high-income countries, the use of basic sanitation facilities, the density of pharmaceutical personnel, and primary school completion rates were found to have significant positive correlations to antibiotic consumption across classes.

*Principle Component Analysis, Variance Inflation Factors, Multiple Linear Regression*
